

# KubeEdge介绍

# 1. KubeEdge简介

`KubeEdge`是基于kubernetes之上将容器化应用的编排能力拓展到边缘主机或边缘设备，在云端和边缘端提供网络通信，应用部署、元数据同步等功能。同时支持**MQTT**协议，允许开发者在边缘端自定义接入边缘设备。

# 2. 功能

- 边缘计算：提供边缘节点自治能力，边缘节点数据处理能力。
- 便捷部署：开发者可以开发http或mqtt协议的应用，运行在云端和边缘端。
- k8s原生支持：可以通过k8s管理和监控边缘设备和边缘节点。
- 丰富的应用类型：可以在边缘端部署机器学习、图片识别、事件处理等应用。

# 3. 组件

## 3.1. 云端

- [CloudHub](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/cloud/cloudhub.md)：一个web socket服务器，负责监听云端的更新、缓存及向`EdgeHub`发送消息。
- [EdgeController](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/cloud/controller.md)：一个扩展的k8s控制器，负责管理边缘节点和pod元数据，同步边缘节点的数据，是`k8s-apiserver` 与`EdgeCore`的通信桥梁。
- [DeviceController](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/cloud/device_controller.md)：一个扩展的k8s控制器，负责管理节点设备，同步云端和边缘端的设备元数据和状态。

## 3.2. 边缘端

- [EdgeHub](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/edgehub.md)：一个web socket客户端，负责云端与边缘端的信息交互，其中包括将云端的资源变更同步到边缘端及边缘端的状态变化同步到云端。
- [Edged](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/edged.md)：运行在边缘节点，管理容器化应用的agent，负责pod生命周期的管理，类似kubelet。
- [EventBus](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/eventbus.md)：一个MQTT客户端，与MQTT服务端交互，提供发布/订阅的能力。
- ServiceBus：一个HTTP客户端，与HTTP服务端交互。为云组件提供HTTP客户端功能，以访问在边缘运行的HTTP服务器。
- [DeviceTwin](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/devicetwin.md)：负责存储设备状态并同步设备状态到云端，同时提供应用的接口查询。
- [MetaManager](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/metamanager.md)：`edged`和`edgehub`之间的消息处理器，负责向轻量数据库（SQLite）存储或查询元数据。

# 4. 架构图

[![kubeedge-arch](https://camo.githubusercontent.com/c1290039d402654f8785b162bed644d3090050dd20cce5e86ee106d2e61e304d/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f647178746e3069636b2f696d6167652f75706c6f61642f76313538303830363234322f61727469636c652f6b756265726e657465732f6b756265656467652f6b756265656467655f617263682e706e67)](https://camo.githubusercontent.com/c1290039d402654f8785b162bed644d3090050dd20cce5e86ee106d2e61e304d/68747470733a2f2f7265732e636c6f7564696e6172792e636f6d2f647178746e3069636b2f696d6167652f75706c6f61642f76313538303830363234322f61727469636c652f6b756265726e657465732f6b756265656467652f6b756265656467655f617263682e706e67)







# KubeEdge源码分析
# cloudcore

# kubeedge源码分析之cloudcore

> 本文源码分析基于[kubeedge v1.1.0](https://github.com/kubeedge/kubeedge/releases/tag/v1.1.0)

本文主要分析cloudcore中CloudCoreCommand的基本流程，具体的`cloudhub`、`edgecontroller`、`devicecontroller`模块的实现逻辑待后续单独文章分析。

目录结构：

> cloud/cmd/cloudcore

```
cloudcore
├── app
│   ├── options
│   │   └── options.go
│   └── server.go # NewCloudCoreCommand、registerModules
└── cloudcore.go # main函数
```



`cloudcore`部分包含以下模块：

- cloudhub
- edgecontroller
- devicecontroller

# 1. main函数

kubeedge的代码采用cobra命令框架，代码风格与k8s源码风格类似。cmd目录主要为cobra command的基本内容及参数解析，pkg目录包含具体的实现逻辑。

> cloud/cmd/cloudcore/cloudcore.go

```
func main() {
	command := app.NewCloudCoreCommand()
	logs.InitLogs()
	defer logs.FlushLogs()

	if err := command.Execute(); err != nil {
		os.Exit(1)
	}
}
```



# 2. NewCloudCoreCommand

`NewCloudCoreCommand`为cobra command的构造函数，该类函数一般包含以下部分：

- 构造option
- 添加Flags
- 运行Run函数（核心）

> cloud/cmd/cloudcore/app/server.go

```
func NewCloudCoreCommand() *cobra.Command {
	opts := options.NewCloudCoreOptions()
	cmd := &cobra.Command{
		Use: "cloudcore",
		Long: `CloudCore is the core cloud part of KubeEdge, which contains three modules: cloudhub,
edgecontroller, and devicecontroller. Cloudhub is a web server responsible for watching changes at the cloud side,
caching and sending messages to EdgeHub. EdgeController is an extended kubernetes controller which manages 
edge nodes and pods metadata so that the data can be targeted to a specific edge node. DeviceController is an extended 
kubernetes controller which manages devices so that the device metadata/status date can be synced between edge and cloud.`,
		Run: func(cmd *cobra.Command, args []string) {
			verflag.PrintAndExitIfRequested()
			flag.PrintFlags(cmd.Flags())

			// To help debugging, immediately log version
			klog.Infof("Version: %+v", version.Get())
			registerModules()
			// start all modules
			core.Run()
		},
	}
	fs := cmd.Flags()
	namedFs := opts.Flags()
	verflag.AddFlags(namedFs.FlagSet("global"))
	globalflag.AddGlobalFlags(namedFs.FlagSet("global"), cmd.Name())
	for _, f := range namedFs.FlagSets {
		fs.AddFlagSet(f)
	}

	usageFmt := "Usage:\n  %s\n"
	cols, _, _ := term.TerminalSize(cmd.OutOrStdout())
	cmd.SetUsageFunc(func(cmd *cobra.Command) error {
		fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine())
		cliflag.PrintSections(cmd.OutOrStderr(), namedFs, cols)
		return nil
	})
	cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) {
		fmt.Fprintf(cmd.OutOrStdout(), "%s\n\n"+usageFmt, cmd.Long, cmd.UseLine())
		cliflag.PrintSections(cmd.OutOrStdout(), namedFs, cols)
	})

	return cmd
}
```



核心代码：

```
// 构造option
opts := options.NewCloudCoreOptions()
// 执行run函数
registerModules()
core.Run()
// 添加flags
fs.AddFlagSet(f)
```



# 3. registerModules

由于kubeedge的代码的大部分模块都采用了基于go-channel的消息通信框架[Beehive](https://kubeedge.readthedocs.io/en/latest/modules/beehive.html)（待后续单独文章分析），因此在各模块启动之前，需要将该模块注册到beehive的框架中。

其中cloudcore部分涉及的模块有：

- cloudhub
- edgecontroller
- devicecontroller

> cloud/cmd/cloudcore/app/server.go

```
// registerModules register all the modules started in cloudcore
func registerModules() {
	cloudhub.Register()
	edgecontroller.Register()
	devicecontroller.Register()
}
```



以下以cloudhub为例说明注册的过程。

cloudhub结构体主要包含：

- context：上下文，用来传递消息上下文
- stopChan：go channel通信

beehive框架中的模块需要实现`Module`接口，因此cloudhub也实现了该接口，其中核心方法为Start，用来启动相应模块的运行。

> vendor/github.com/kubeedge/beehive/pkg/core/module.go

```
// Module interface
type Module interface {
	Name() string
	Group() string
	Start(c *context.Context)
	Cleanup()
}
```



以下为cloudHub结构体及注册函数。

> cloud/pkg/cloudhub/cloudhub.go

```
type cloudHub struct {
	context  *context.Context
	stopChan chan bool
}

func Register() {
	core.Register(&cloudHub{})
}
```



具体的注册实现函数为core.Register，注册过程实际上就是将具体的模块结构体放入一个以模块名为key的map映射中，待后续调用。

> vendor/github.com/kubeedge/beehive/pkg/core/module.go

```
// Register register module
func Register(m Module) {
	if isModuleEnabled(m.Name()) {
		modules[m.Name()] = m  //将具体的模块结构体放入一个以模块名为key的map映射中
		log.LOGGER.Info("module " + m.Name() + " registered")
	} else {
		disabledModules[m.Name()] = m
		log.LOGGER.Info("module " + m.Name() +
			" is not register, please check modules.yaml")
	}
}
```



# 4. core.Run

CloudCoreCommand命令的Run函数实际上是运行beehive框架中注册的所有模块。

其中包括两部分逻辑：

- 启动运行所有注册模块
- 监听信号并做优雅清理

> vendor/github.com/kubeedge/beehive/pkg/core/core.go

```
//Run starts the modules and in the end does module cleanup
func Run() {
	//Address the module registration and start the core
	StartModules()
	// monitor system signal and shutdown gracefully
	GracefulShutdown()
}
```



# 5. StartModules

`StartModules`获取context上下文，并以goroutine的方式运行所有已注册的模块。其中Start函数即每个模块的具体实现`Module`接口中的Start方法。不同模块各自定义自己的具体Start方法实现。

```
coreContext := context.GetContext(context.MsgCtxTypeChannel)
go module.Start(coreContext)
```



具体实现如下：

> vendor/github.com/kubeedge/beehive/pkg/core/core.go

```
// StartModules starts modules that are registered
func StartModules() {
	coreContext := context.GetContext(context.MsgCtxTypeChannel)

	modules := GetModules()
	for name, module := range modules {
		//Init the module
		coreContext.AddModule(name)
		//Assemble typeChannels for send2Group
		coreContext.AddModuleGroup(name, module.Group())
		go module.Start(coreContext)
		log.LOGGER.Info("starting module " + name)
	}
}
```



# 6. GracefulShutdown

当收到相关信号，则执行各个模块实现的Cleanup方法。

> vendor/github.com/kubeedge/beehive/pkg/core/core.go

```
// GracefulShutdown is if it gets the special signals it does modules cleanup
func GracefulShutdown() {
	c := make(chan os.Signal)
	signal.Notify(c, syscall.SIGINT, syscall.SIGHUP, syscall.SIGTERM,
		syscall.SIGQUIT, syscall.SIGILL, syscall.SIGTRAP, syscall.SIGABRT)
	select {
	case s := <-c:
		log.LOGGER.Info("got os signal " + s.String())
		//Cleanup each modules
		modules := GetModules()
		for name, module := range modules {
			log.LOGGER.Info("Cleanup module " + name)
			module.Cleanup()
		}
	}
}
```

# edgecore

# kubeedge源码分析之edgecore

> 本文源码分析基于[kubeedge v1.1.0](https://github.com/kubeedge/kubeedge/releases/tag/v1.1.0)

本文主要分析`edgecore`中`EdgeCoreCommand`的基本流程，具体的`edged`、`edgehub`、`metamanager`等模块的实现逻辑待后续单独文章分析。

目录结构：

```
edgecore
├── app
│   ├── options
│   │   └── options.go
│   └── server.go  # NewEdgeCoreCommand 、registerModules
└── edgecore.go  # main
```



edgecore模块包含：

- edged
- edgehub
- metamanager
- eventbus
- servicebus
- devicetwin
- edgemesh

# 1. main函数

main入口函数，仍然是cobra命令框架格式。

> edge/cmd/edgecore/edgecore.go

```
func main() {
	command := app.NewEdgeCoreCommand()
	logs.InitLogs()
	defer logs.FlushLogs()

	if err := command.Execute(); err != nil {
		os.Exit(1)
	}
}
```



# 2. NewEdgeCoreCommand

`NewEdgeCoreCommand`与`NewCloudCoreCommand`一样构造对应的cobra command结构体。

> edge/cmd/edgecore/app/server.go

```
// NewEdgeCoreCommand create edgecore cmd
func NewEdgeCoreCommand() *cobra.Command {
	opts := options.NewEdgeCoreOptions()
	cmd := &cobra.Command{
		Use: "edgecore",
		Long: `Edgecore is the core edge part of KubeEdge, which contains six modules: devicetwin, edged, 
edgehub, eventbus, metamanager, and servicebus. DeviceTwin is responsible for storing device status 
and syncing device status to the cloud. It also provides query interfaces for applications. Edged is an 
agent that runs on edge nodes and manages containerized applications and devices. Edgehub is a web socket 
client responsible for interacting with Cloud Service for the edge computing (like Edge Controller as in the KubeEdge 
Architecture). This includes syncing cloud-side resource updates to the edge, and reporting 
edge-side host and device status changes to the cloud. EventBus is a MQTT client to interact with MQTT 
servers (mosquito), offering publish and subscribe capabilities to other components. MetaManager 
is the message processor between edged and edgehub. It is also responsible for storing/retrieving metadata 
to/from a lightweight database (SQLite).ServiceBus is a HTTP client to interact with HTTP servers (REST), 
offering HTTP client capabilities to components of cloud to reach HTTP servers running at edge. `,
		Run: func(cmd *cobra.Command, args []string) {
			verflag.PrintAndExitIfRequested()
			flag.PrintFlags(cmd.Flags())

			// To help debugging, immediately log version
			klog.Infof("Version: %+v", version.Get())

			registerModules()
			// start all modules
			core.Run()
		},
	}
	fs := cmd.Flags()
	namedFs := opts.Flags()
	verflag.AddFlags(namedFs.FlagSet("global"))
	globalflag.AddGlobalFlags(namedFs.FlagSet("global"), cmd.Name())
	for _, f := range namedFs.FlagSets {
		fs.AddFlagSet(f)
	}

	usageFmt := "Usage:\n  %s\n"
	cols, _, _ := term.TerminalSize(cmd.OutOrStdout())
	cmd.SetUsageFunc(func(cmd *cobra.Command) error {
		fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine())
		cliflag.PrintSections(cmd.OutOrStderr(), namedFs, cols)
		return nil
	})
	cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) {
		fmt.Fprintf(cmd.OutOrStdout(), "%s\n\n"+usageFmt, cmd.Long, cmd.UseLine())
		cliflag.PrintSections(cmd.OutOrStdout(), namedFs, cols)
	})

	return cmd
}
```



核心代码：

```
opts := options.NewEdgeCoreOptions()
registerModules()
core.Run()
```



# 3. registerModules

edgecore仍然采用[Beehive](https://kubeedge.readthedocs.io/en/latest/modules/beehive.html)通信框架，模块调用前先注册对应的模块。具体参考[cloudcore.registerModules](https://www.huweihuang.com/kubernetes-notes/kubeedge/code-analysis/cloudcore.html#3-registermodules)处的分析，此处不再展开分析注册流程。此处注册的是edgecore中涉及的组件。

> edge/cmd/edgecore/app/server.go

```
// registerModules register all the modules started in edgecore
func registerModules() {
	devicetwin.Register()
	edged.Register()
	edgehub.Register()
	eventbus.Register()
	edgemesh.Register()
	metamanager.Register()
	servicebus.Register()
	test.Register()
	dbm.InitDBManager()
}
```



# 4. core.Run

core.Run与[cloudcore.run](https://www.huweihuang.com/kubernetes-notes/kubeedge/code-analysis/cloudcore.html#4-corerun)处逻辑一致不再展开分析。

> vendor/github.com/kubeedge/beehive/pkg/core/core.go

```
//Run starts the modules and in the end does module cleanup
func Run() {
   //Address the module registration and start the core
   StartModules()
   // monitor system signal and shutdown gracefully
   GracefulShutdown()
}
```

# OpenYurt
# OpenYurt部署

> 本文主要介绍部署openyurt组件到k8s集群中。

## 1. 给云端节点和边缘节点打标签

openyurt将k8s节点分为云端节点和边缘节点，云端节点主要运行一些云端的业务，边缘节点运行边缘业务。当与 `apiserver` 断开连接时，只有运行在边缘自治的节点上的Pod才不会被驱逐。通过打 `openyurt.io/is-edge-worker` 的标签的方式来区分，`false`表示云端节点，`true`表示边缘节点。

云端组件：

- yurt-controller-manager
- yurt-tunnel-server

边缘组件：

- yurt-hub
- yurt-tunnel-agent

### 1.1. openyurt.io/is-edge-worker节点标签

```
# 云端节点，值为false
kubectl label node us-west-1.192.168.0.87 openyurt.io/is-edge-worker=false

# 边缘节点，值为true
kubectl label node us-west-1.192.168.0.88 openyurt.io/is-edge-worker=true
```



### 1.2. 给边缘节点开启自治模式

```
kubectl annotate node us-west-1.192.168.0.88 node.beta.openyurt.io/autonomy=true
```



## 2. 安装准备

### 2.1. 调整k8s组件的配置

参考[调整k8s组件的配置](https://blog.huweihuang.com/kubernetes-notes/edge/openyurt/update-k8s-for-openyurt/)

### 2.2. 部署tunnel-dns

```
wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurt-tunnel-dns.yaml
kubectl apply -f yurt-tunnel-dns.yaml
```



获取clusterIP，作为kube-apiserver的专用nameserver地址。

```
kubectl -n kube-system get svc yurt-tunnel-dns -o=jsonpath='{.spec.clusterIP}'
```



## 3. 部署openyurt控制面

通过helm来部署控制面，所有helm charts都可以在[openyurt-helm 仓库](https://github.com/openyurtio/openyurt-helm)中找到。

快捷安装可参考脚本：[helm-install-openyurt.sh](https://github.com/huweihuang/kubeadm-scripts/blob/main/openyurt/cloud/helm-install-openyurt.sh)

```
helm repo add openyurt https://openyurtio.github.io/openyurt-helm
```



### 3.1. yurt-app-manager

```
helm upgrade --install yurt-app-manager -n kube-system openyurt/yurt-app-manager
```



### 3.2. openyurt

在`openyurt/openyurt`中的组件包括：

- [yurt-controller-manager](https://openyurt.io/zh/docs/core-concepts/yurt-controller-manager): 防止apiserver在断开连接时驱逐运行在边缘节点上的pod
- [yurt-tunnel-server](https://openyurt.io/zh/docs/core-concepts/yurttunnel): 在云端构建云边隧道
- [yurt-tunnel-agent](https://openyurt.io/zh/docs/core-concepts/yurttunnel): 在边缘侧构建云边隧道

由于yurt-tunnel-server默认使用host模式，因此可能存在边缘端的agent无法访问云端的tunnel-server，需要为tunnel-server配置一个可访问的地址。

```
# 下载并解压
helm pull openyurt/openyurt --untar

# 修改tunnel相关配置
cd openyurt 
vi values.yaml

# 示例：
yurtTunnelServer:
  replicaCount: 1
  tolerations: []
  parameters:
    certDnsNames: "<tunnel server的域名>"
    tunnelAgentConnectPort: <tunnel server端口，默认为10262>
    certIps: ""


yurtTunnelAgent:
  replicaCount: 1
  tolerations: []
  parameters:
    tunnelserverAddr: "<tunnel server的地址，包括端口>"


# install
helm install openyurt ./openyurt
```



## 4. 部署 Yurthub(edge)

在 `yurt-controller-manager` 启动并正常运行后，以静态 pod 的方式部署 `Yurthub`。

1. 为 yurthub 创建全局配置(即RBAC, configmap)

```
wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurthub-cfg.yaml
kubectl apply -f yurthub-cfg.yaml
```



1. 在边缘节点以static pod方式创建yurthub

```
mkdir -p /etc/kubernetes/manifests/
cd /etc/kubernetes/manifests/
wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurthub.yaml 

# 获取bootstrap token
kubeadm token create

# 假设 apiserver 的地址是 1.2.3.4:6443，bootstrap token 是 07401b.f395accd246ae52d
sed -i 's|__kubernetes_master_address__|1.2.3.4:6443|;
s|__bootstrap_token__|07401b.f395accd246ae52d|' /etc/kubernetes/manifests/yurthub.yaml
```



## 5. 重置 Kubelet

重置 kubelet 服务，让它通过 yurthub 访问apiserver。为 kubelet 服务创建一个新的 kubeconfig 文件来访问apiserver。

```
mkdir -p /var/lib/openyurt
cat << EOF > /var/lib/openyurt/kubelet.conf
apiVersion: v1
clusters:
- cluster:
    server: http://127.0.0.1:10261
  name: default-cluster
contexts:
- context:
    cluster: default-cluster
    namespace: default
    user: default-auth
  name: default-context
current-context: default-context
kind: Config
preferences: {}
EOF
```



修改`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`

```
sed -i "s|KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=\/etc\/kubernetes\/bootstrap-kubelet.conf\ --kubeconfig=\/etc\/kubernetes\/kubelet.conf|KUBELET_KUBECONFIG_ARGS=--kubeconfig=\/var\/lib\/openyurt\/kubelet.conf|g" \
    /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
```



重启kubelet服务

```
systemctl daemon-reload && systemctl restart kubelet
```



## 6. yurthub部署脚本

根据以上部署步骤，整理部署脚本。需要修改脚本内容的`master-addr`和`token`字段。

```
#!/bin/bash
set -e
set -x

### install yurthub ###
mkdir -p /etc/kubernetes/manifests/
cd /etc/kubernetes/manifests/
wget https://raw.githubusercontent.com/openyurtio/openyurt/master/config/setup/yurthub.yaml 


### 修改master和token字段
sed -i 's|__kubernetes_master_address__|<master-addr>:6443|;
s|__bootstrap_token__|<token>|' /etc/kubernetes/manifests/yurthub.yaml

mkdir -p /var/lib/openyurt
cat << EOF > /var/lib/openyurt/kubelet.conf
apiVersion: v1
clusters:
- cluster:
    server: http://127.0.0.1:10261
  name: default-cluster
contexts:
- context:
    cluster: default-cluster
    namespace: default
    user: default-auth
  name: default-context
current-context: default-context
kind: Config
preferences: {}
EOF

cp /etc/systemd/system/kubelet.service.d/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.bak
sed -i "s|KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=\/etc\/kubernetes\/bootstrap-kubelet.conf\ --kubeconfig=\/etc\/kubernetes\/kubelet.conf|KUBELET_KUBECONFIG_ARGS=--kubeconfig=\/var\/lib\/openyurt\/kubelet.conf|g" \
    /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl daemon-reload && systemctl restart kubelet
```

# OpenYurt部署之调整k8s配置

安装openyurt，为了适配边缘场景，需要对k8s组件进行调整。其中包括：

- kube-apiserver
- kube-controller-manager
- kube-proxy
- CoreDNS

# 1. kube-apiserver

为了实现云边通信，即用户可以正常使用kubectl exec/logs的功能来登录或查看边缘容器的信息。需要将kube-apiserver访问kubelet的地址调整为hostname优先。

```
$ vi /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
...
spec:
  dnsPolicy: "None" # 1. dnsPolicy修改为None
  dnsConfig:        # 2. 增加dnsConfig配置
    nameservers:
      - 1.2.3.4 # 使用yurt-tunnel-dns service的clusterIP替换
    searches:
      - kube-system.svc.cluster.local
      - svc.cluster.local
      - cluster.local
    options:
      - name: ndots
        value: "5"
  containers:
  - command:
    - kube-apiserver
  ...
    - --kubelet-preferred-address-types=Hostname,InternalIP,ExternalIP # 3. 把Hostname放在第一位
  ...
```



# 2. kube-controller-manager

禁用默认的 `nodelifecycle` 控制器，当节点断连时不驱逐pod。

`nodelifecycle`控制器主要用来根据node的status及lease的更新时间来决定是否要`驱逐节点上的pod`。为了让 `yurt-controller-mamanger` 能够正常工作，因此需要禁用controller的驱逐功能。

```
vim /etc/kubernetes/manifests/kube-controller-manager.yaml
# 在--controllers=*,bootstrapsigner,tokencleaner后面添加,-nodelifecycle 
# 即参数为： --controllers=*,bootstrapsigner,tokencleaner,-nodelifecycle

# 如果kube-controller-manager是以static pod部署，修改yaml文件后会自动重启。
```



# 3. CoreDNS

将coredns从deployment部署改为daemonset部署。

将deployment的coredns副本数调整为0。

```
kubectl scale --replicas=0 deployment/coredns -n kube-system
```



创建daemonset的coredns。

```
wget https://raw.githubusercontent.com/huweihuang/kubeadm-scripts/main/openyurt/yurt-tunnel/coredns.ds.yaml

kubectl apply -f 
```



支持流量拓扑：

```
# 利用openyurt实现endpoint过滤
kubectl annotate svc kube-dns -n kube-system openyurt.io/topologyKeys='openyurt.io/nodepool'
```



# 4. kube-proxy

云边端场景下，边缘节点间很有可能无法互通，因此需要endpoints基于nodepool进行拓扑。直接将kube-proxy的kubeconfig配置删除，将apiserver请求经过yurthub即可解决服务拓扑问题。

```
kubectl edit cm -n kube-system kube-proxy
```



示例：

```
apiVersion: v1
data:
  config.conf: |-
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    bindAddress: 0.0.0.0
    bindAddressHardFail: false
    clientConnection:
      acceptContentTypes: ""
      burst: 0
      contentType: ""
      #kubeconfig: /var/lib/kube-proxy/kubeconfig.conf <-- 删除这个配置
      qps: 0
    clusterCIDR: 100.64.0.0/10
    configSyncPeriod: 0s
// 省略
```



参考：