# 存储卷概念

# Volume

# 1. volume概述

- 容器上的文件生命周期同容器的生命周期一致，即容器挂掉之后，容器将会以最初镜像中的文件系统内容启动，之前容器运行时产生的文件将会丢失。
- Pod的volume的生命周期同Pod的生命周期一致，当Pod被删除的时候，对应的volume才会被删除。即Pod中的容器重启时，之前的文件仍可以保存。

容器中的进程看到的是由其 `Docker 镜像和卷`组成的文件系统视图。

**Pod volume的使用方式**

Pod 中的每个容器都必须独立指定每个卷的挂载位置，需要给Pod配置volume相关参数。

Pod的volume关键字段如下：

- spec.volumes：提供怎样的数据卷
- spec.containers.volumeMounts：挂载到容器的什么路径

# 2. volume类型

## 2.1. emptyDir

**1、特点**

- 会创建`emptyDir`对应的目录，默认为空（如果该目录原来有文件也会被重置为空）
- Pod中的不同容器可以在目录中读写相同文件（即Pod中的不同容器可以通过该方式来共享文件）
- 当Pod被删除，`emptyDir` 中的数据将被永久删除，如果只是Pod挂掉该数据还会保留

**2、使用场景**

- 不同容器之间共享文件（例如日志采集等）

- 暂存空间，例如用于基于磁盘的合并排序

- 用作长时间计算崩溃恢复时的检查点

  **3、示例**

```
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
```



## 2.2. hostPath

**1、特点**

- 会将宿主机的目录或文件挂载到Pod中

**2、使用场景**

- 运行需要访问 Docker 内部的容器；使用 `/var/lib/docker` 的 `hostPath`
- 在容器中运行 cAdvisor；使用 `/dev/cgroups` 的 `hostPath`
- 其他使用到宿主机文件的场景

**`hostPath`的`type`字段**

| 值                  | 行为                                                         |
| ------------------- | ------------------------------------------------------------ |
|                     | 空字符串（默认）用于向后兼容，这意味着在挂载 hostPath 卷之前不会执行任何检查。 |
| `DirectoryOrCreate` | 如果在给定的路径上没有任何东西存在，那么将根据需要在那里创建一个空目录，权限设置为 0755，与 Kubelet 具有相同的组和所有权。 |
| `Directory`         | 给定的路径下必须存在目录                                     |
| `FileOrCreate`      | 如果在给定的路径上没有任何东西存在，那么会根据需要创建一个空文件，权限设置为 0644，与 Kubelet 具有相同的组和所有权。 |
| `File`              | 给定的路径下必须存在文件                                     |
| `Socket`            | 给定的路径下必须存在 UNIX 套接字                             |
| `CharDevice`        | 给定的路径下必须存在字符设备                                 |
| `BlockDevice`       | 给定的路径下必须存在块设备                                   |

**注意事项**

- 由于每个节点上的文件都不同，具有相同配置的 pod 在不同节点上的行为可能会有所不同
- 当 Kubernetes 按照计划添加资源感知调度时，将无法考虑 `hostPath` 使用的资源
- 在底层主机上创建的文件或目录只能由 root 写入。您需要在[特权容器](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)中以 root 身份运行进程，或修改主机上的文件权限以便写入 `hostPath` 卷

**3、示例**

```
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
```



### 2.2.1. 同步文件变化到容器内

`hostPath`的挂载方式可以挂载目录和文件两种格式，如果使用文件挂载的方式，通过简单的`vi`等命令修改宿主机的文件，并不会实时同步到容器内的映射文件。而需要对容器进行重启的操作才可以把文件的修改内容同步到文件中，但生产的容器一般不建议执行重启的操作。因此我们可以通过以下的方式来避免这个问题的发生。

以上文件不同步的本质原因是容器在初次挂载的时候使用了宿主机的文件的inode number进行标识，而**vi等操作会导致文件的inode number发生变化**，所以当宿主机文件的inode number发生变化，容器内并不会发生变化，**为了保持文件内容一致，则需要保持修改文件的同时文件的inode number不变**。那么我们**可以使用 cat 或echo 命令覆盖文件的内容则inode number不会发生变化。**

示例：

```
      containers:
        volumeMounts:
        - mountPath: /etc/hosts
          name: hosts
          readOnly: true
      volumes:
      - hostPath:
          path: /etc/hosts
          type: FileOrCreate
        name: hosts
```



例如，以上的案例是通过挂载宿主机的/etc/hosts文件来映射到容器，如果想修改宿主机的hosts文件来同步容器内的hosts文件，可以通过以下的方式:

```
# 查看文件的inode
ls -i /etc/hosts
39324780 /etc/hosts

# 追加记录
echo "1.1.1.1 xxx.com" >> /etc/hosts

# 替换内容
sed 's/1.1.1.1/2.2.2.2/g' /etc/hosts > temp.txt
cat temp.txt > /etc/hosts

# 查看宿主机和容器内的inode号都没有发生变化
# crictl exec -it 20891de31a4a6 sh
/var/www/html # ls -i /etc/hosts
39324780 /etc/hosts
```



## 2.3. configMap

`configMap`提供了一种给Pod注入配置文件的方式，配置文件内容存储在configMap对象中，如果Pod使用configMap作为volume的类型，需要先创建configMap的对象。

**示例**

```
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level
```



## 2.4. cephfs

`cephfs`的方式将Pod的存储挂载到`ceph`集群中，通过外部存储的方式持久化Pod的数据（即当Pod被删除数据仍可以存储在ceph集群中），前提是先部署和维护好一个ceph集群。

**示例**

```
apiVersion: v1
kind: Pod
metadata:
  name: cephfs
spec:
  containers:
  - name: cephfs-rw
    image: kubernetes/pause
    volumeMounts:
    - mountPath: "/mnt/cephfs"
      name: cephfs
  volumes:
  - name: cephfs
    cephfs:
      monitors:
      - 10.16.154.78:6789
      - 10.16.154.82:6789
      - 10.16.154.83:6789
      # by default the path is /, but you can override and mount a specific path of the filesystem by using the path attribute
      # path: /some/path/in/side/cephfs 
      user: admin
      secretFile: "/etc/ceph/admin.secret"
      readOnly: true
```



更多可参考 [CephFS 示例](https://github.com/kubernetes/examples/tree/master/staging/volumes/cephfs/)。

## 2.5. nfs

`nfs`的方式类似cephfs，即将Pod数据存储到NFS集群中，具体可参考[NFS示例](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs)。

## 2.6. persistentVolumeClaim

`persistentVolumeClaim` 卷用于将`PersistentVolume`挂载到容器中。PersistentVolumes 是在用户不知道特定云环境的细节的情况下“声明”持久化存储（例如 GCE PersistentDisk 或 iSCSI 卷）的一种方式。

# Persistent Volume

## 1. PV概述

`PersistentVolume`（简称PV） 是 Volume 之类的卷插件，也是集群中的资源，但独立于Pod的生命周期（即不会因Pod删除而被删除），不归属于某个Namespace。

## 2. PV和PVC的生命周期

### 2.1. 配置（Provision）

有两种方式来配置 PV：静态或动态。

**1、静态**

手动创建PV，可供k8s集群中的对象消费。

**2、动态**

可以通过`StorageClass`和具体的`Provisioner`（例如`nfs-client-provisioner`）来动态地创建和删除PV。

### 2.2. 绑定

在动态配置的情况下，用户创建了特定的PVC，k8s会监听新的PVC，并寻找匹配的PV绑定。一旦绑定后，这种绑定是排他性的，PVC和PV的绑定是一对一的映射。

### 2.3. 使用

Pod 使用PVC作为卷。集群检查PVC以查找绑定的卷并为集群挂载该卷。用户通过在 Pod 的 volume 配置中包含 `persistentVolumeClaim` 来调度 Pod 并访问用户声明的 PV。

### 2.4. 回收

PV的回收策略可以设定PVC在释放后如何处理对应的Volume，目前有 `Retained`， `Recycled `和` Deleted`三种策略。

**1、保留**（Retain）

保留策略允许手动回收资源，当删除PVC的时候，PV仍然存在，可以通过以下步骤回收卷：

1. 删除PV
2. 手动清理外部存储的数据资源
3. 手动删除或重新使用关联的存储资产

**2、回收**（Resycle）

> 该策略已废弃，推荐使用dynamic provisioning

回收策略会在 volume上执行基本擦除（`rm -rf / thevolume / *`），可被再次声明使用。

**3、删除**（Delete）

删除策略，当发生删除操作的时候，会从k8s集群中删除PV对象，并执行外部存储资源的删除操作（根据不同的provisioner定义的删除逻辑不同，有的是重命名）。

动态配置的卷继承其`StorageClass`的回收策略，默认为Delete，即当用户删除PVC的时候，会自动执行PV的删除策略。

如果要修改PV的回收策略，可执行以下命令：

```
# Get pv 
kubectl get pv
# Change policy to Retaion
kubectl patch pv <pv_name> -p ‘{“spec”:{“persistentVolumeReclaimPolicy”:“Retain”}}’
```



## 3. PV的类型

`PersistentVolume` 类型以插件形式实现。以下仅列部分常用类型：

- GCEPersistentDisk
- AWSElasticBlockStore
- NFS
- RBD (Ceph Block Device)
- CephFS
- Glusterfs

## 4. PV的属性

每个` PV` 配置中都包含一个 `sepc `规格字段和一个 `status` 卷状态字段。

```
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: fuseim.pri/ifs
  creationTimestamp: 2018-07-12T06:46:48Z
  name: default-test-web-0-pvc-58cf5ec1-859f-11e8-bb61-005056b83985
  resourceVersion: "100163256"
  selfLink: /api/v1/persistentvolumes/default-test-web-0-pvc-58cf5ec1-859f-11e8-bb61-005056b83985
  uid: 59796ba3-859f-11e8-9c50-c81f66bcff65
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 2Gi
  volumeMode: Filesystem  
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: test-web-0
    namespace: default
    resourceVersion: "100163248"
    uid: 58cf5ec1-859f-11e8-bb61-005056b83985
  nfs:
    path: /data/nfs-storage/default-test-web-0-pvc-58cf5ec1-859f-11e8-bb61-005056b83985
    server: 172.16.201.54
  persistentVolumeReclaimPolicy: Delete
  storageClassName: managed-nfs-storage
  mountOptions:
    - hard
    - nfsvers=4.1
status:
  phase: Bound
```



### 4.1. Capacity

给PV设置特定的`存储容量`，更多 `capacity` 可参考Kubernetes [资源模型](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md) 。

### 4.2. Volume Mode

` volumeMode` 的有效值可以是`Filesystem`或`Block`。如果未指定，volumeMode 将默认为`Filesystem`。

### 4.3. Access Modes

访问模式包括：

- `ReadWriteOnce`——该卷可以被单个节点以读/写模式挂载
- `ReadOnlyMany`——该卷可以被多个节点以只读模式挂载
- `ReadWriteMany`——该卷可以被多个节点以读/写模式挂载

在命令行中，访问模式缩写为：

- RWO - ReadWriteOnce
- ROX - ReadOnlyMany
- RWX - ReadWriteMany

> 一个卷一次只能使用一种访问模式挂载，即使它支持很多访问模式。

以下只列举部分常用插件：

| Volume 插件          | ReadWriteOnce | ReadOnlyMany | ReadWriteMany |
| -------------------- | ------------- | ------------ | ------------- |
| AWSElasticBlockStore | ✓             | -            | -             |
| CephFS               | ✓             | ✓            | ✓             |
| GCEPersistentDisk    | ✓             | ✓            | -             |
| Glusterfs            | ✓             | ✓            | ✓             |
| HostPath             | ✓             | -            | -             |
| NFS                  | ✓             | ✓            | ✓             |
| RBD                  | ✓             | ✓            | -             |
| ...                  |               |              | -             |

### 4.4. Class

`PV`可以指定一个`StorageClass`来动态绑定PV和PVC，其中通过 `storageClassName` 属性来指定具体的`StorageClass`，如果没有指定该属性的PV，它只能绑定到不需要特定类的 PVC。

### 4.5. Reclaim Policy

回收策略包括：

- `Retain`（保留）——手动回收
- `Recycle`（回收）——基本擦除（`rm -rf /thevolume/*`）
- `Delete`（删除）——关联的存储资产（例如 AWS EBS、GCE PD、Azure Disk 和 OpenStack Cinder 卷）将被删除

当前，只有 NFS 和 HostPath 支持回收策略。AWS EBS、GCE PD、Azure Disk 和 Cinder 卷支持删除策略。

### 4.6. Mount Options

Kubernetes 管理员可以指定在节点上为挂载持久卷指定挂载选项。

> **注意**：不是所有的持久化卷类型都支持挂载选项。

支持挂载选项常用的类型有：

- GCEPersistentDisk
- AWSElasticBlockStore
- AzureFile
- AzureDisk
- NFS
- RBD （Ceph Block Device）
- CephFS
- Cinder （OpenStack 卷存储）
- Glusterfs

### 4.7. Phase

PV可以处于以下的某种状态：

- `Available`（可用）——一块空闲资源还没有被任何声明绑定
- `Bound`（已绑定）——卷已经被声明绑定
- `Released`（已释放）——声明被删除，但是资源还未被集群重新声明
- `Failed`（失败）——该卷的自动回收失败

命令行会显示绑定到 PV 的 PVC 的名称。

# Persistent Volume Claim

## 1. PVC概述

`PersistentVolumeClaim`（简称PVC）是用户存储的请求，PVC消耗PV的资源，可以请求特定的大小和访问模式，需要指定归属于某个Namespace，在同一个Namespace的Pod才可以指定对应的PVC。

当需要不同性质的PV来满足存储需求时，可以使用`StorageClass`来实现。

每个 PVC 中都包含一个 spec 规格字段和一个 status 声明状态字段。

```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels:
      release: "stable"
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}
```



## 2. PVC的属性

### 2.1. accessModes

对应存储的访问模式，例如：`ReadWriteOnce`。

### 2.2. volumeMode

对应存储的数据卷模式，例如：`Filesystem`。

### 2.3. resources

声明可以请求特定数量的资源。相同的[资源模型](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md)适用于Volume和PVC。

### 2.4. selector

声明`label selector`，只有标签与选择器匹配的卷可以绑定到声明。

- matchLabels：volume 必须有具有该值的标签
- matchExpressions：条件列表，通过条件表达式筛选匹配的卷。有效的运算符包括 In、NotIn、Exists 和 DoesNotExist。

### 2.5. storageClassName

通过`storageClassName`参数来指定使用对应名字的`StorageClass`，只有所请求的类与 PVC 具有相同 `storageClassName` 的 PV 才能绑定到 PVC。

PVC可以不指定storageClassName，或者将该值设置为空，如果打开了准入控制插件，并且指定一个默认的 `StorageClass`，则PVC会使用默认的`StorageClass`，否则就绑定到没有`StorageClass`的 PV上。

> 之前使用注解 `volume.beta.kubernetes.io/storage-class` 而不是 `storageClassName` 属性。这个注解仍然有效，但是在未来的 Kubernetes 版本中不会支持。

## 3. 将PVC作为Volume

将PVC作为Pod的Volume，PVC与Pod需要在同一个命名空间下，其实Pod的声明如下：

```
kind: Pod
apiVersion: v1
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: dockerfile/nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:    # 使用PVC
        claimName: myclaim
```



`PersistentVolumes` 绑定是唯一的，并且由于 `PersistentVolumeClaims` 是命名空间对象，因此只能在一个命名空间内挂载具有“多个”模式（`ROX`、`RWX`）的PVC。

# Storage Class

## 1. StorageClass概述

`StorageClass`提供了一种描述`存储类`（class）的方法，不同的class可能会映射到不同的服务质量等级和备份策略或其他策略等。

`StorageClass` 对象中包含 `provisioner`、`parameters` 和 `reclaimPolicy` 字段，当需要动态分配 `PersistentVolume` 时会使用到。当创建 `StorageClass` 对象时，设置名称和其他参数，一旦创建了对象就不能再对其更新。也可以为没有申请绑定到特定 class 的 PVC 指定一个默认的 `StorageClass` 。

**StorageClass对象文件**

```
kind: StorageClass
apiVersion: storage.k8s.io/v3
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
mountOptions:
  - debug
```



## 2. StorageClass的属性

### 2.1. Provisioner（存储分配器）

Storage class 有一个`分配器（provisioner）`，用来决定使用哪个卷插件分配 PV，该字段必须指定。可以指定内部分配器，也可以指定外部分配器。外部分配器的代码地址为： [kubernetes-incubator/external-storage](https://github.com/kubernetes-incubator/external-storage)，其中包括`NFS`和`Ceph`等。

### 2.2. Reclaim Policy（回收策略）

可以通过`reclaimPolicy`字段指定创建的`Persistent Volume`的回收策略，回收策略包括：`Delete` 或者 `Retain`，没有指定默认为`Delete`。

### 2.3. Mount Options（挂载选项）

由 storage class 动态创建的 Persistent Volume 将使用 class 中 `mountOptions` 字段指定的挂载选项。

### 2.4. 参数

Storage class 具有描述属于 storage class 卷的参数。取决于`分配器`，可以接受不同的参数。 当参数被省略时，会使用默认值。

例如以下使用`Ceph RBD`

```
kind: StorageClass
apiVersion: storage.k8s.io/v3
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 30.36.353.305:6789
  adminId: kube
  adminSecretName: ceph-secret
  adminSecretNamespace: kube-system
  pool: kube
  userId: kube
  userSecretName: ceph-secret-user
  fsType: ext4
  imageFormat: "2"
  imageFeatures: "layering"
```



**对应的参数说明**

- `monitors`：Ceph monitor，逗号分隔。该参数是必需的。

- `adminId`：Ceph 客户端 ID，用于在池（ceph pool）中创建映像。 默认是 “admin”。

- `adminSecretNamespace`：adminSecret 的 namespace。默认是 “default”。

- `adminSecret`：adminId 的 Secret 名称。该参数是必需的。 提供的 secret 必须有值为 “kubernetes.io/rbd” 的 type 参数。

- `pool`: Ceph RBD 池. 默认是 “rbd”。

- `userId`：Ceph 客户端 ID，用于映射 RBD 镜像（RBD image）。默认与 adminId 相同。

- `userSecretName`：用于映射 RBD 镜像的 userId 的 Ceph Secret 的名字。 它必须与 PVC 存在于相同的 namespace 中。该参数是必需的。 提供的 secret 必须具有值为 “kubernetes.io/rbd” 的 type 参数，例如以这样的方式创建：

  ```
  kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \
    --from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==' \
    --namespace=kube-system
  ```

  

- `fsType`：Kubernetes 支持的 fsType。默认："ext4"。

- `imageFormat`：Ceph RBD 镜像格式，”1” 或者 “2”。默认值是 “1”。

- `imageFeatures`：这个参数是可选的，只能在你将 imageFormat 设置为 “2” 才使用。 目前支持的功能只是 `layering`。 默认是 ““，没有功能打开。

# Dynamic Volume Provisioning

## Dynamic Volume Provisioning

Dynamic volume provisioning允许用户按需自动创建存储卷，这种方式可以让用户不需要关心存储的复杂性和差别，又可以选择不同的存储类型。

## 1. 开启Dynamic Provisioning

需要先提前创建`StorageClass`对象，`StorageClass`中定义了使用哪个`provisioner`，并且在`provisioner`被调用时传入哪些参数，具体可参考`StorageClass`介绍。

例如：

- 磁盘类存储

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
```



- SSD类存储

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
```



## 2. 使用Dynamic Provisioning

创建一个PVC对象，并且在其中`storageClassName`字段指明需要用到的`StorageClass`的名称，例如：

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  resources:
    requests:
      storage: 30Gi
```



当使用到PVC的时候会自动创建对应的外部存储，当PVC被删除的时候，会自动销毁（或备份）外部存储。

## 3. 默认的StorageClass

当没有对应的`StorageClass`配置时，可以设定默认的`StorageClass`，需要执行以下操作：

- 在API Server开启[`DefaultStorageClass` admission controller](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass) 。
- 设置默认的`StorageClass`对象。

可以通过添加`storageclass.kubernetes.io/is-default-class`注解的方式设置某个`StorageClass`为默认的`StorageClass`。当用户创建了一个`PersistentVolumeClaim`，但没有指定`storageClassName`的时候，会自动将该PVC的`storageClassName`指向默认的`StorageClass`。

# CSI

# csi-cephfs-plugin

# 1. 编译CSI CephFS plugin

CSI CephFS plugin用来提供CephFS存储卷和挂载存储卷，源码参考：https://github.com/ceph/ceph-csi 。

## 1.1. 编译二进制

```
$ make cephfsplugin
```



## 1.2. 编译Docker镜像

```
$ make image-cephfsplugin
```



# 2. 配置项

## 2.1. 命令行参数

| Option            | Default value         | Description                                                  |
| ----------------- | --------------------- | ------------------------------------------------------------ |
| `--endpoint`      | `unix://tmp/csi.sock` | CSI endpoint, must be a UNIX socket                          |
| `--drivername`    | `csi-cephfsplugin`    | name of the driver (Kubernetes: `provisioner` field in StorageClass must correspond to this value) |
| `--nodeid`        | *empty*               | This node’s ID                                               |
| `--volumemounter` | *empty*               | default volume mounter. Available options are `kernel` and `fuse`. This is the mount method used if volume parameters don’t specify otherwise. If left unspecified, the driver will first probe for `ceph-fuse` in system’s path and will choose Ceph kernel client if probing failed. |

## 2.2. volume参数

| Parameter                                                    | Required                    | Description                                                  |
| ------------------------------------------------------------ | --------------------------- | ------------------------------------------------------------ |
| `monitors`                                                   | yes                         | Comma separated list of Ceph monitors (e.g. `192.168.100.1:6789,192.168.100.2:6789,192.168.100.3:6789`) |
| `mounter`                                                    | no                          | Mount method to be used for this volume. Available options are `kernel` for Ceph kernel client and `fuse` for Ceph FUSE driver. Defaults to “default mounter”, see command line arguments. |
| `provisionVolume`                                            | yes                         | Mode of operation. BOOL value. If `true`, a new CephFS volume will be provisioned. If `false`, an existing CephFS will be used. |
| `pool`                                                       | for `provisionVolume=true`  | Ceph pool into which the volume shall be created             |
| `rootPath`                                                   | for `provisionVolume=false` | Root path of an existing CephFS volume                       |
| `csiProvisionerSecretName`, `csiNodeStageSecretName`         | for Kubernetes              | name of the Kubernetes Secret object containing Ceph client credentials. Both parameters should have the same value |
| `csiProvisionerSecretNamespace`, `csiNodeStageSecretNamespace` | for Kubernetes              | namespaces of the above Secret objects                       |

## 2.3. provisionVolume

### 2.3.1. 管理员密钥认证

当**provisionVolume=true**时，必要的管理员认证参数如下：

- `adminID`: ID of an admin client
- `adminKey`: key of the admin client

### 2.3.2. 普通用户密钥认证

当**provisionVolume=false**时，必要的用户认证参数如下：

- `userID`: ID of a user client
- `userKey`: key of a user client

# 部署csi-cephfs

# 0. 说明

要求Kubernetes的版本在`1.11`及以上，k8s集群必须允许特权Pod（`privileged pods`），即apiserver和kubelet需要设置`--allow-privileged`为`true`。节点的`Docker daemon`需要允许挂载共享卷。

## 涉及镜像

- quay.io/k8scsi/csi-provisioner:v0.3.0
- quay.io/k8scsi/csi-attacher:v0.3.0
- quay.io/k8scsi/driver-registrar:v0.3.0
- quay.io/cephcsi/cephfsplugin:v0.3.0

# 1. 部署RBAC

部署`service accounts`, `cluster roles` 和 `cluster role bindings`，这些可供`RBD`和`CephFS CSI plugins`共同使用，他们拥有相同的权限。

```
$ kubectl create -f csi-attacher-rbac.yaml
$ kubectl create -f csi-provisioner-rbac.yaml
$ kubectl create -f csi-nodeplugin-rbac.yaml
```



## 1.1. csi-attacher-rbac.yaml

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-attacher

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: external-attacher-runner
rules:
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["volumeattachments"]
    verbs: ["get", "list", "watch", "update"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-attacher-role
subjects:
  - kind: ServiceAccount
    name: csi-attacher
    namespace: default
roleRef:
  kind: ClusterRole
  name: external-attacher-runner
  apiGroup: rbac.authorization.k8s.io
```



## 1.2. csi-provisioner-rbac.yaml

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-provisioner

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: external-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
    
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-provisioner-role
subjects:
  - kind: ServiceAccount
    name: csi-provisioner
    namespace: default
roleRef:
  kind: ClusterRole
  name: external-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
```



## 1.3. csi-nodeplugin-rbac.yaml

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-nodeplugin

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-nodeplugin
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "update"]
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["get", "list"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["volumeattachments"]
    verbs: ["get", "list", "watch", "update"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-nodeplugin
subjects:
  - kind: ServiceAccount
    name: csi-nodeplugin
    namespace: default
roleRef:
  kind: ClusterRole
  name: csi-nodeplugin
  apiGroup: rbac.authorization.k8s.io          
```



# 2. 部署CSI sidecar containers

通过`StatefulSet`的方式部署`external-attacher`和`external-provisioner`供`CSI CephFS`使用。

```
$ kubectl create -f csi-cephfsplugin-attacher.yaml
$ kubectl create -f csi-cephfsplugin-provisioner.yaml
```



## 2.1. csi-cephfsplugin-provisioner.yaml

```
kind: Service
apiVersion: v1
metadata:
  name: csi-cephfsplugin-provisioner
  labels:
    app: csi-cephfsplugin-provisioner
spec:
  selector:
    app: csi-cephfsplugin-provisioner
  ports:
    - name: dummy
      port: 12345

---
kind: StatefulSet
apiVersion: apps/v1beta1
metadata:
  name: csi-cephfsplugin-provisioner
spec:
  serviceName: "csi-cephfsplugin-provisioner"
  replicas: 1
  template:
    metadata:
      labels:
        app: csi-cephfsplugin-provisioner
    spec:
      serviceAccount: csi-provisioner
      containers:
        - name: csi-provisioner
          image: quay.io/k8scsi/csi-provisioner:v0.3.0
          args:
            - "--provisioner=csi-cephfsplugin"
            - "--csi-address=$(ADDRESS)"
            - "--v=5"
          env:
            - name: ADDRESS
              value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock
          imagePullPolicy: "IfNotPresent"
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin
      volumes:
        - name: socket-dir
          hostPath:
            path: /var/lib/kubelet/plugins/csi-cephfsplugin
            type: DirectoryOrCreate
```



## 2.2. csi-cephfsplugin-attacher.yaml

```
kind: Service
apiVersion: v1
metadata:
  name: csi-cephfsplugin-attacher
  labels:
    app: csi-cephfsplugin-attacher
spec:
  selector:
    app: csi-cephfsplugin-attacher
  ports:
    - name: dummy
      port: 12345

---
kind: StatefulSet
apiVersion: apps/v1beta1
metadata:
  name: csi-cephfsplugin-attacher
spec:
  serviceName: "csi-cephfsplugin-attacher"
  replicas: 1
  template:
    metadata:
      labels:
        app: csi-cephfsplugin-attacher
    spec:
      serviceAccount: csi-attacher
      containers:
        - name: csi-cephfsplugin-attacher
          image: quay.io/k8scsi/csi-attacher:v0.3.0
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
          env:
            - name: ADDRESS
              value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock
          imagePullPolicy: "IfNotPresent"
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin
      volumes:
        - name: socket-dir
          hostPath:
            path: /var/lib/kubelet/plugins/csi-cephfsplugin
            type: DirectoryOrCreate
```



# 3. 部署CSI-CephFS-driver(plugin)

> csi-cephfs-plugin 的作用类似nfs-client，部署在所有node节点上，执行ceph的挂载等相关任务。

通过`DaemonSet`的方式部署，其中包括两个容器：`CSI driver-registrar` 和 `CSI CephFS driver`。

```
$ kubectl create -f csi-cephfsplugin.yaml
```



## 3.1. csi-cephfsplugin.yaml

```
kind: DaemonSet
apiVersion: apps/v1beta2
metadata:
  name: csi-cephfsplugin
spec:
  selector:
    matchLabels:
      app: csi-cephfsplugin
  template:
    metadata:
      labels:
        app: csi-cephfsplugin
    spec:
      serviceAccount: csi-nodeplugin
      hostNetwork: true
      # to use e.g. Rook orchestrated cluster, and mons' FQDN is
      # resolved through k8s service, set dns policy to cluster first
      dnsPolicy: ClusterFirstWithHostNet      
      containers:
        - name: driver-registrar
          image: quay.io/k8scsi/driver-registrar:v0.3.0
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
            - "--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)"
          env:
            - name: ADDRESS
              value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock
            - name: DRIVER_REG_SOCK_PATH
              value: /var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin
            - name: registration-dir
              mountPath: /registration
        - name: csi-cephfsplugin
          securityContext:
            privileged: true
            capabilities:
              add: ["SYS_ADMIN"]
            allowPrivilegeEscalation: true
          image: quay.io/cephcsi/cephfsplugin:v0.3.0
          args :
            - "--nodeid=$(NODE_ID)"
            - "--endpoint=$(CSI_ENDPOINT)"
            - "--v=5"
            - "--drivername=csi-cephfsplugin"
          env:
            - name: NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: CSI_ENDPOINT
              value: unix://var/lib/kubelet/plugins/csi-cephfsplugin/csi.sock
          imagePullPolicy: "IfNotPresent"
          volumeMounts:
            - name: plugin-dir
              mountPath: /var/lib/kubelet/plugins/csi-cephfsplugin
            - name: pods-mount-dir
              mountPath: /var/lib/kubelet/pods
              mountPropagation: "Bidirectional"
            - mountPath: /sys
              name: host-sys
            - name: lib-modules
              mountPath: /lib/modules
              readOnly: true
            - name: host-dev
              mountPath: /dev
      volumes:
        - name: plugin-dir
          hostPath:
            path: /var/lib/kubelet/plugins/csi-cephfsplugin
            type: DirectoryOrCreate
        - name: registration-dir
          hostPath:
            path: /var/lib/kubelet/plugins/
            type: Directory
        - name: pods-mount-dir
          hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
        - name: socket-dir
          hostPath:
            path: /var/lib/kubelet/plugins/csi-cephfsplugin
            type: DirectoryOrCreate
        - name: host-sys
          hostPath:
            path: /sys
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: host-dev
          hostPath:
            path: /dev
```



# 4. 确认部署结果

```
$ kubectl get all
NAME                                 READY     STATUS    RESTARTS   AGE
pod/csi-cephfsplugin-attacher-0      1/1       Running   0          26s
pod/csi-cephfsplugin-provisioner-0   1/1       Running   0          25s
pod/csi-cephfsplugin-rljcv           2/2       Running   0          24s

NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
service/csi-cephfsplugin-attacher      ClusterIP   10.104.116.218   <none>        12345/TCP   27s
service/csi-cephfsplugin-provisioner   ClusterIP   10.101.78.75     <none>        12345/TCP   26s

...
```

# 部署cephfs-provisioner

# 1. 安装cephfs客户端

所有node节点安装cephfs客户端，主要用来和ceph集群挂载使用。

```
yum install -y ceph-common
```



# 2. 部署RBAC

## 2.1. ClusterRole

```
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cephfs-provisioner
  namespace: cephfs
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["kube-dns","coredns"]
    verbs: ["list", "get"]
```



## 2.2. ClusterRoleBinding

```
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cephfs-provisioner
subjects:
  - kind: ServiceAccount
    name: cephfs-provisioner
    namespace: cephfs
roleRef:
  kind: ClusterRole
  name: cephfs-provisioner
  apiGroup: rbac.authorization.k8s.io
```



## 2.3. Role

```
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cephfs-provisioner
  namespace: cephfs
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["create", "get", "delete"]
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
```



## 2.4. RoleBinding

```
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cephfs-provisioner
  namespace: cephfs
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cephfs-provisioner
subjects:
- kind: ServiceAccount
  name: cephfs-provisioner
```



## 2.5. ServiceAccount

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cephfs-provisioner
  namespace: cephfs
```



# 3. 部署 cephfs-provisioner

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cephfs-provisioner
  namespace: cephfs
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: cephfs-provisioner
    spec:
      containers:
      - name: cephfs-provisioner
        image: "quay.io/external_storage/cephfs-provisioner:latest"
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 64Mi        
        env:
        - name: PROVISIONER_NAME                # 与storageclass的provisioner参数相同
          value: ceph.com/cephfs
        - name: PROVISIONER_SECRET_NAMESPACE    # 与rbac的namespace相同
          value: cephfs
        command:
        - "/usr/local/bin/cephfs-provisioner"
        args:
        - "-id=cephfs-provisioner-1"
      serviceAccount: cephfs-provisioner
```



# 4. 部署storageclass

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: cephfs-provisioner-sc
provisioner: ceph.com/cephfs
volumeBindingMode: WaitForFirstConsumer
parameters:
  monitors: 192.168.27.43:6789,192.168.27.44:6789,192.168.27.45:6789
  adminId: admin
  adminSecretName: csi-cephfs-secret
  adminSecretNamespace: "kube-csi"
  claimRoot: /pvc-volumes
```



# 5. 部署statefulset

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cephfs-provisioner-nginx
spec:
  serviceName: "nginx"
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest   #nginx的镜像
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - mountPath: "/mnt"      #容器里面的挂载目录，该目录挂载到NFS的共享目录上
          name: test
  volumeClaimTemplates:
  - metadata:
      name: test
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 2Gi
      storageClassName: cephfs-provisioner-sc
```



# 6. 日志

## 6.1. cephfs-provisoner 执行日志

```
I0327 07:18:19.742239       1 controller.go:987] provision "default/test-cephfs-ngx-wait-22-0" class "cephfs-provisioner-sc": started
I0327 07:18:19.745239       1 event.go:221] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"test-cephfs-ngx-wait-22-0", UID:"7f6b60d5-5060-11e9-9a9c-c81f66bcff65", APIVersion:"v1", ResourceVersion:"347214256", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/test-cephfs-ngx-wait-22-0"
I0327 07:18:23.281277       1 cephfs-provisioner.go:222] successfully created CephFS share &CephFSPersistentVolumeSource{Monitors:[192.168.27.43:6789 192.168.27.44:6789 192.168.27.45:6789],Path:/pvc-volumes/kubernetes/kubernetes-dynamic-pvc-7f7cb62f-5060-11e9-85c0-0adb8ef08100,User:kubernetes-dynamic-user-7f7cb69f-5060-11e9-85c0-0adb8ef08100,SecretFile:,SecretRef:&SecretReference{Name:ceph-kubernetes-dynamic-user-7f7cb69f-5060-11e9-85c0-0adb8ef08100-secret,Namespace:default,},ReadOnly:false,}
I0327 07:18:23.281371       1 controller.go:1087] provision "default/test-cephfs-ngx-wait-22-0" class "cephfs-provisioner-sc": volume "pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65" provisioned
I0327 07:18:23.281415       1 controller.go:1101] provision "default/test-cephfs-ngx-wait-22-0" class "cephfs-provisioner-sc": trying to save persistentvvolume "pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65"
I0327 07:18:23.284621       1 controller.go:1108] provision "default/test-cephfs-ngx-wait-22-0" class "cephfs-provisioner-sc": persistentvolume "pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65" saved
I0327 07:18:23.284723       1 controller.go:1149] provision "default/test-cephfs-ngx-wait-22-0" class "cephfs-provisioner-sc": succeeded
I0327 07:18:23.284810       1 event.go:221] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"test-cephfs-ngx-wait-22-0", UID:"7f6b60d5-5060-11e9-9a9c-c81f66bcff65", APIVersion:"v1", ResourceVersion:"347214256", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-7f6b60d5-5060-11e9-9a9c-c81f66bcff65
```



## 6.2. debug 日志

```
I0327 08:08:11.789608       1 controller.go:987] provision "default/test-cephfs-ngx-wait-44-0" class "cephfs-sc-wait": started
I0327 08:08:11.793258       1 event.go:221] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"test-cephfs-ngx-wait-44-0", UID:"81846859-5067-11e9-9a9c-c81f66bcff65", APIVersion:"v1", ResourceVersion:"347237916", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/test-cephfs-ngx-wait-44-0"
E0327 08:08:12.164705       1 cephfs-provisioner.go:158] failed to provision share "kubernetes-dynamic-pvc-76ecdc5a-5067-11e9-9421-2a1b1be1aeef" for "kubernetes-dynamic-user-76ecdcee-5067-11e9-9421-2a1b1be1aeef", err: exit status 1, output: Traceback (most recent call last):
  File "/usr/local/bin/cephfs_provisioner", line 364, in <module>
    main()
  File "/usr/local/bin/cephfs_provisioner", line 358, in main
    print cephfs.create_share(share, user, size=size)
  File "/usr/local/bin/cephfs_provisioner", line 228, in create_share
    volume = self.volume_client.create_volume(volume_path, size=size, namespace_isolated=not self.ceph_namespace_isolation_disabled)
  File "/usr/local/bin/cephfs_provisioner", line 112, in volume_client
    self._volume_client.connect(None)
  File "/lib/python2.7/site-packages/ceph_volume_client.py", line 458, in connect
    self.rados.connect()
  File "rados.pyx", line 895, in rados.Rados.connect (/home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos7/DIST/centos7/MACHINE_SIZE/huge/release/13.2.1/rpm/el7/BUILD/ceph-13.2.1/build/src/pybind/rados/pyrex/rados.c:9815)
rados.IOError: [errno 5] error connecting to the cluster
W0327 08:08:12.164908       1 controller.go:746] Retrying syncing claim "default/test-cephfs-ngx-wait-44-0" because failures 2 < threshold 15
E0327 08:08:12.164977       1 controller.go:761] error syncing claim "default/test-cephfs-ngx-wait-44-0": failed to provision volume with StorageClass "cephfs-sc-wait": exit status 1
I0327 08:08:12.165974       1 event.go:221] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"test-cephfs-ngx-wait-44-0", UID:"81846859-5067-11e9-9a9c-c81f66bcff65", APIVersion:"v1", ResourceVersion:"347237916", FieldPath:""}): type: 'Warning' reason: 'ProvisioningFailed' failed to provision volume with StorageClass "cephfs-sc-wait": exit status 1
```

# FlexVolume介绍

# 1. FlexVolume介绍

Flexvolume提供了一种扩展k8s存储插件的方式，用户可以自定义自己的存储插件。类似的功能的实现还有CSI的方式。Flexvolume在k8s 1.8+以上版本提供GA功能版本。

# 2. 使用方式

在每个node节点安装存储插件二进制，该二进制实现flexvolume的相关接口，默认存储插件的存放路径为`/usr/libexec/kubernetes/kubelet-plugins/volume/exec/<vendor~driver>/<driver>`。

其中`vendor~driver`的名字需要和pod中flexVolume.driver的字段名字匹配，该字段名字通过`/`替换`~`。

例如：

- path:/usr/libexec/kubernetes/kubelet-plugins/volume/exec/foo~cifs/cifs
- pod中flexVolume.driver:foo/cifs

# 3. FlexVolume接口

节点上的存储插件需要实现以下的接口。

## 3.1. init

```
<driver executable> init
```



## 3.2. attach

```
<driver executable> attach <json options> <node name>
```



## 3.3. detach

```
<driver executable> detach <mount device> <node name>
```



## 3.4. waitforattach

```
<driver executable> waitforattach <mount device> <json options>
```



## 3.5. isattached

```
<driver executable> isattached <json options> <node name>
```



## 3.6. mountdevice

```
<driver executable> mountdevice <mount dir> <mount device> <json options>
```



## 3.7. unmountdevice

```
<driver executable> unmountdevice <mount device>
```



## 3.8. mount

```
<driver executable> mount <mount dir> <json options>
```



## 3.9. unmount

```
<driver executable> unmount <mount dir>
```



## 3.10. 插件输出

```
{
	"status": "<Success/Failure/Not supported>",
	"message": "<Reason for success/failure>",
	"device": "<Path to the device attached. This field is valid only for attach & waitforattach call-outs>"
	"volumeName": "<Cluster wide unique name of the volume. Valid only for getvolumename call-out>"
	"attached": <True/False (Return true if volume is attached on the node. Valid only for isattached call-out)>
    "capabilities": <Only included as part of the Init response>
    {
        "attach": <True/False (Return true if the driver implements attach and detach)>
    }
}
```



# 4. 示例

## 4.1. pod的yaml文件内容

**nginx-nfs.yaml**

相关参数为flexVolume.driver等。

```
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nfs
  namespace: default
spec:
  containers:
  - name: nginx-nfs
    image: nginx
    volumeMounts:
    - name: test
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: test
    flexVolume:
      driver: "k8s/nfs"
      fsType: "nfs"
      options:
        server: "172.16.0.25"
        share: "dws_nas_scratch"
```



## 4.2. 插件脚本

nfs脚本实现了flexvolume的接口。

/usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs。

```
#!/bin/bash

# Copyright 2015 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Notes:
#  - Please install "jq" package before using this driver.
usage() {
	err "Invalid usage. Usage: "
	err "\t$0 init"
	err "\t$0 mount <mount dir> <json params>"
	err "\t$0 unmount <mount dir>"
	exit 1
}

err() {
	echo -ne $* 1>&2
}

log() {
	echo -ne $* >&1
}

ismounted() {
	MOUNT=`findmnt -n ${MNTPATH} 2>/dev/null | cut -d' ' -f1`
	if [ "${MOUNT}" == "${MNTPATH}" ]; then
		echo "1"
	else
		echo "0"
	fi
}

domount() {
	MNTPATH=$1

	NFS_SERVER=$(echo $2 | jq -r '.server')
	SHARE=$(echo $2 | jq -r '.share')

	if [ $(ismounted) -eq 1 ] ; then
		log '{"status": "Success"}'
		exit 0
	fi

	mkdir -p ${MNTPATH} &> /dev/null

	mount -t nfs ${NFS_SERVER}:/${SHARE} ${MNTPATH} &> /dev/null
	if [ $? -ne 0 ]; then
		err "{ \"status\": \"Failure\", \"message\": \"Failed to mount ${NFS_SERVER}:${SHARE} at ${MNTPATH}\"}"
		exit 1
	fi
	log '{"status": "Success"}'
	exit 0
}

unmount() {
	MNTPATH=$1
	if [ $(ismounted) -eq 0 ] ; then
		log '{"status": "Success"}'
		exit 0
	fi

	umount ${MNTPATH} &> /dev/null
	if [ $? -ne 0 ]; then
		err "{ \"status\": \"Failed\", \"message\": \"Failed to unmount volume at ${MNTPATH}\"}"
		exit 1
	fi

	log '{"status": "Success"}'
	exit 0
}

op=$1

if ! command -v jq >/dev/null 2>&1; then
	err "{ \"status\": \"Failure\", \"message\": \"'jq' binary not found. Please install jq package before using this driver\"}"
	exit 1
fi

if [ "$op" = "init" ]; then
	log '{"status": "Success", "capabilities": {"attach": false}}'
	exit 0
fi

if [ $# -lt 2 ]; then
	usage
fi

shift

case "$op" in
	mount)
		domount $*
		;;
	unmount)
		unmount $*
		;;
	*)
		log '{"status": "Not supported"}'
		exit 0
esac

exit 1
```